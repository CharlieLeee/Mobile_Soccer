{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Soccer Mobile\n",
    "<!-- Introduction of the project -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Students: Chengkun Li, Jiangfan Li, Georgios Apostolides, Lucas Represa**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## I. Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this project the goal is to combine vision, path planning, local navigation, and filtering in order to make Thymio robot navigate trough a map towards a goal.  \n",
    "\n",
    "As the constraints are established we were free to chose our own implementation. To begin with, a camera is used in order to provide the vision information. The vision module works along with the global path and filter modules extracting estimating and computing the necessary map information, including the robot pose, robot pose, map, static obstacles, and the goal position. \n",
    "\n",
    "Indeed, a Kalman filter performs the estimations of the robot pose. Afterwards, the A* algorithm computes the optimal path. Following this, a global controller gives instructions to the motors for them to follow the optimal path. Finally, a local navigation module is implemented in order to provide a vision-free information to Thymio for the most spontaneous events such as a dynamic obstacle."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## II. Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Import Libraries**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%config InlineBackend.figure_format = 'retina'\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import time\n",
    "import random\n",
    "from vision import *\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.rcParams['figure.figsize'] = [20, 10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Thymio Connection**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tdmclient.notebook\n",
    "await tdmclient.notebook.start()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Environment**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to create our soccer field environnement we printed an A1 sheet with the corresponding field. Corners are represented by [...]. A camera performs the vision part by transferring frames to the vision module. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "G_verbose = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Geometry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from geo import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Vision\n",
    "\n",
    "\n",
    "The vision module is responsible for giving the coordinates for different objects of interest. Specifically, the vision module outputs the position of the ball in the field and a map of the obstacles. Additionally, the vision part is responsible for identifying the corners of the field and wrapping the image so that a top planar view of the field can be achieved.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A) Using Images of the Setup\n",
    "\n",
    "In this section we explain the operations and functions performed from the vision module using image. This is done in case while running this notebook you don't have the environment setup or the camera in hand."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vp = VisionProcessor()\n",
    "img_thymio = cv2.imread(\"img/example.jpg\")\n",
    "plt.imshow(cv2.cvtColor(img_thymio, cv2.COLOR_RGB2BGR))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Align the Field"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Color Filtering\n",
    "\n",
    "In order to fascilitate the use of color filtering a function called `color_filter` under the class `VisionProcessor` was createrd. The values of various colors in the HSV color space were preset in order to make the use of color filtering easier. Specifically the following colors where defined so far: red, green, blue, yellow, black, pink, white. If the color doesn't much the above an error is outputted saying so.\n",
    "\n",
    "`color_filtered_image = VisionProcessor.color_filter(img, color=color)`\n",
    "\n",
    "\n",
    "\n",
    "- **Output:**\n",
    "        Color filtered Image\n",
    "\n",
    "- **Arguments:**\n",
    "\n",
    "| Argument |                                          Explanation                                          |\n",
    "|:--------:|:---------------------------------------------------------------------------------------------:|\n",
    "|   image  |<p style=\"text-align: left;\"> image to be color filtered</p>|\n",
    "|   color  |<p style=\"text-align: left;\"> color in which to filter the image (i.e.red, green, blue, yellow, black, pink, white)</p> |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "color = 'green'\n",
    "green_mask = VisionProcessor.color_filter(img_thymio, color=color)\n",
    "plt.imshow(cv2.cvtColor(green_mask, cv2.COLOR_RGB2BGR))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Corner Detection\n",
    "\n",
    "This section demonstrates to methods which were developed for identifying the corners of the field. The first method uses image processing to detect the corners of the field while the second one uses aruco markers which represents the corners of the field."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 1. Using Image Processing\n",
    "\n",
    "Although this method was developed it is not used in the project as the corner identification was proven to be noisy and to vary with light changes. Nevertheless the method is left here for future reference. \n",
    "\n",
    "The method uses the result of color filtering to identify the biggest contour by taking the convex hull of the largest contours. The four corners are then identified using the means of a Gaussian Mixture Model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. find the biggest green part\n",
    "\n",
    "gray = cv2.cvtColor(green_mask, cv2.COLOR_BGR2GRAY)\n",
    "(T, thresh) = cv2.threshold(gray, 0, 255, cv2.THRESH_OTSU)\n",
    "# morphology operation against noise\n",
    "closing = cv2.morphologyEx(thresh, cv2.MORPH_CLOSE, np.ones((10, 10)))\n",
    "# extract contours\n",
    "_,contours,hierarchy = cv2.findContours(closing,cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "\n",
    "# Taking the one with the largest area # serve as the opening operation to some extent\n",
    "c = max(contours, key = cv2.contourArea)\n",
    "# use a convexHull against noise\n",
    "hull = cv2.convexHull(c)\n",
    "hull_img = np.zeros_like(img)     \n",
    "length = len(hull)\n",
    "for i in range(len(hull)):\n",
    "    cv2.line(hull_img, tuple(hull[i][0]), tuple(hull[(i+1)%length][0]), (255,0,0), 2)\n",
    "hull_img = cv2.cvtColor(hull_img, cv2.COLOR_RGB2GRAY)\n",
    "plt.imshow(hull_img)    \n",
    "_,convexcontour,_ = cv2.findContours(hull_img,cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "ch = img.copy()\n",
    "cv2.drawContours(ch,convexcontour,-1,(255,0,0),1)\n",
    "#plt.imshow(cv2.cvtColor(ch, cv2.COLOR_RGB2BGR))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. get 4 corners\n",
    "lines = VisionProcessor.divide4(np.array(hull))\n",
    "ll = lines[np.argmax([abs(l[-1]) for l in lines])]\n",
    "lines.remove(ll)\n",
    "rl = lines[np.argmax([abs(l[-1]) for l in lines])]\n",
    "lines.remove(rl)\n",
    "ul = lines[0]\n",
    "dl = lines[1]\n",
    "corners = []\n",
    "i = 1\n",
    "ch = img.copy()\n",
    "for h in [ul, dl]:\n",
    "    for v in [ll, rl]:\n",
    "        x, y = VisionProcessor.intersection(h[0], h[1], -h[2], v[0], v[1], -v[2])\n",
    "        corners.append([x, y])\n",
    "        print(\"corner\",x, y)\n",
    "        cv2.circle(ch, (x, y), 3, (0, 255, 0), -1)\n",
    "        cv2.putText(ch,str(i), (x,y), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 255), 2, cv2.LINE_AA)\n",
    "        i += 1\n",
    "        \n",
    "corners = np.array(corners)\n",
    "\n",
    "plt.imshow(ch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2. Using aruco\n",
    "\n",
    "\n",
    "To fascilitate the detection of the corners and be invariant to lightiing conditions aruco markers where used at each corner to signify it. By detecting the aruco markers position we are able to detect the pixel position on the image which represents the corner. \n",
    "\n",
    "Below, you can see an example from a loaded image:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_obs = cv2.imread(\"img/test.jpg\")\n",
    "plt.imshow(cv2.cvtColor(img_obs, cv2.COLOR_RGB2BGR))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corners, a_image = vp.visualize_aruco(img_obs)\n",
    "plt.imshow(cv2.cvtColor(a_image, cv2.COLOR_RGB2BGR))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Perspective Projection\n",
    "\n",
    "The perspective projection section explains how the vision modules takes the four identified corners and returns a planar projection of the field. \n",
    "\n",
    "_`align_field`:_ This function which is part of the `VisionProcessor` class is responsible for taking the 4 corners identified before using the aruco markers and returning the perspecive transform `M` between the skewed plane of the field and the rectangle which is formed using the maximum Height and maximum width. To get the perpective transform a function from the OpenCV library is used called `cv2.getPerspectiveTransform` which uses the identified corners and the constructed rectangle corners.\n",
    "\n",
    "`M = VisionProcessor.align_field(corners)`\n",
    "\n",
    "\n",
    "\n",
    "- **Output:**\n",
    "        Perspective Transformation Matrix (M)\n",
    "\n",
    "- **Arguments:**\n",
    "\n",
    "| Argument |             Explanation            |\n",
    "|:--------:|:----------------------------------:|\n",
    "|  corners | corners array in pixel coordinates |\n",
    "\n",
    "\n",
    "_`warp`:_ This function which is part of the `VisionProcessor` class is responsible for taking an image and applying the previously calculated transformation. This is done by warping the image using the `cv2.wrapPerspective` function from the OpenCV library.\n",
    "\n",
    "`warped = VisionProcessor.warp(img, M)`\n",
    "\n",
    "- **Output:**\n",
    "        Warped image (warped)\n",
    "\n",
    "- **Arguments:**\n",
    "\n",
    "| Argument |                          Explanation                         |\n",
    "|:--------:|:------------------------------------------------------------:|\n",
    "|    img   |<p style=\"text-align: left;\">original image which will be wrapped with perspective matrix </p>|\n",
    "|     M    |<p style=\"text-align: left;\">perspective transformation matrix </p>                          |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Corners' Pixel Coordinates:\")\n",
    "M = vp.align_field(corners)\n",
    "wraped = VisionProcessor.warp(img_obs, M)\n",
    "plt.imshow(cv2.cvtColor(wraped, cv2.COLOR_RGB2BGR))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get Thymio's Pose"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Box Detection\n",
    "\n",
    "One of the expected outputs from the vision class is the pose of the robot. In order to be able to identify the pose of the robot two rectangular colored squares were placed on top of thymio. By finding the center of each of the boxes and connecting them we get a vector pointing to the direction where thymio is traveling.\n",
    "\n",
    "\n",
    "`(cx,cy) = VisionProcessor.detect_box(image,color='yellow',verbose=False)`\n",
    "\n",
    "\n",
    "- **Output:**\n",
    "        Cartesian Pixel Coordinates for the center of the box to be detected.\n",
    "        \n",
    "- **Arguments:**\n",
    "\n",
    "| Argument |                         Explanation                         |\n",
    "|:--------:|:-----------------------------------------------------------:|\n",
    "|   image  |<p style=\"text-align: left;\">image on which the box will be detected</p>|\n",
    "|   color  |<p style=\"text-align: left;\">color of the box to be filtered. </p>|\n",
    "| verbose  |<p style=\"text-align: left;\">Used for displaying the intermediate steps while debugging.</p>|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(cv2.cvtColor(img_thymio, cv2.COLOR_BGR2RGB))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(cx_blue,cy_blue)=VisionProcessor.detect_box(img_thymio,color='blue',verbose=False)\n",
    "(cx_yellow,cy_yellow)=VisionProcessor.detect_box(img_thymio,color='yellow',verbose=False)\n",
    "cv2.circle(img_thymio,(cx_blue,cy_blue),1,(0,255,0),12);\n",
    "cv2.circle(img_thymio,(cx_yellow,cy_yellow),1,(0,255,0),12);\n",
    "plt.imshow(cv2.cvtColor(img_thymio, cv2.COLOR_BGR2RGB))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get Robot's Pose\n",
    "\n",
    "_`get_robot_pose`:_ Receives as an input a frame which contains the robot. It uses the `detect_box` function demonstrated above to find the centers of the boxes on thymio. Then it uses them to define the Pose of thymion     (x, y, $\\theta$ ) using the displacement from the origin. The pose of the robot is scaled into physical distances using the FieldScale.\n",
    "\n",
    "`(robot_xy,robot_angle) = get_robot_pose(image, verbose = False)`\n",
    "\n",
    "- **Output:**\n",
    "   Robot Cartesian Position in physical distances and robots orientation in radians.\n",
    "\n",
    "- **Arguments:**\n",
    "\n",
    "| Argument |                         Explanation                         |\n",
    "|:--------:|:-----------------------------------------------------------:|\n",
    "|   image  |<p style=\"text-align: left;\">image on which the robot will be detected</p>|\n",
    "| verbose  |<p style=\"text-align: left;\">Used for displaying the intermediate steps while debugging.</p>|\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_thymio2 = cv2.imread(\"img/example.jpg\")\n",
    "\n",
    "#Drawing State vector on a new image\n",
    "state = VisionProcessor.get_robot_pose(img_thymio, verbose = False)\n",
    "cv2.arrowedLine(img_thymio2,(cx_blue,cy_blue), (cx_yellow,cy_yellow), color=(0,255,0), thickness=8)\n",
    "cv2.circle(img_thymio2,(cx_blue,cy_blue),1,(0,0,255),12);\n",
    "\n",
    "#Outputing the state of the robot\n",
    "print(\"     X            Y                  Theta \\n\",state)\n",
    "plt.imshow(cv2.cvtColor(img_thymio2, cv2.COLOR_BGR2RGB))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ball Detection\n",
    "\n",
    "_`get_ball_xy`:_ Part of the class Vision processor used to identify the position of the ball in the field. It receives as an input an image of the field with the ball. The image is first color filtered based on the `color` argument passed. In our case we use the green color the reason is because the ball is circular at the botom it leaves a black circle on the green surface which can be identified. Thresholding and a median filder for smoothing is applied on the color filtered image. A function from the OpenCV library is used to detect circles in the image. More information for the `cv2.HoughCircles` can be found [here](https://docs.opencv.org/4.x/dd/d1a/group__imgproc__feature.html#ga47849c3be0d0406ad3ca45db65a25d2d) and [here](https://docs.opencv.org/3.4/d4/d70/tutorial_hough_circle.html). The function returns the XY pixel coordinates of the identified circle.\n",
    "\n",
    "``ball_pos=VisionProcessor.get_ball_xy(wraped,color = \"green\", minDist = 850, param1 = 12,param2 = 22,``\\\n",
    "&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;``minRadius = 20,maxRadius = 55,verbose = True)``\n",
    "\n",
    "- **Outputs:**\n",
    "        Used to output the position of the ball in the field in pixel coordinates.\n",
    "- **Arguments:**\n",
    "\n",
    "|  Argument |<p style=\"text-align: center;\">                                                                   Explanation      </p>                                                    |\n",
    "|:---------:|:---------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n",
    "|   image   | image on which the box will be detected                                                                                                                               |\n",
    "|   color   | color of the box to be filtered                                                                                                                                       |\n",
    "| minDist   | Minimum distance between the centers of the detected circles. If too large circles may be missed, if too small too many circles may appear.                           |\n",
    "| param1    | Method uses canny filter edge detection.This is the higher threshold of the two passed to the Canny edge detector (the lower one is twice smaller of the larger one). |\n",
    "| param2    | Accumulator threshold for the circle centers at the detection stage.The smaller it is, the more false circles may be detected.                                        |\n",
    "| minRadius | Min radius of circle to detect                                                                                                                                        |\n",
    "| maxRadius | Max radius of circle to detect                                                                                                                                        |\n",
    "| verbose   | Used for displaying the intermediate steps while debugging.                                                                                                           |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ball_img=wraped.copy()\n",
    "# wraped = VisionProcessor.warp(ball_img, M)\n",
    "\n",
    "ball_pos=VisionProcessor.get_ball_xy(ball_img,color = \"green\", minDist = 850, param1 = 12,param2 = 22,\n",
    "                                     minRadius = 20,maxRadius = 55,verbose = True)\n",
    "cv2.circle(ball_img,ball_pos,1,(0,0,255),12);\n",
    "plt.imshow(cv2.cvtColor(ball_img, cv2.COLOR_RGB2BGR))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build a Map\n",
    "\n",
    "_`obstacle_map`:_ This function is used to color filter obstacles in the `color` passed as argument and form a binary map of the field. The function receives as input the image in which the obstacles need to be detected. A Gaussian Blur filter is applied to smoothened the image. The scene is colored filtered with the given color argument in our case with the pink color. Erossion and dilation is applied to remove noise from the map.\n",
    "\n",
    "`map = VisionProcessor.obstacles_map(image, color = 'pink', blur_kernel = (19, 19), verbose = False)`\n",
    "\n",
    "\n",
    "- **Output:**\n",
    "        Binary Map of the image passed. Obstacles Represented in black color (map)\n",
    "        \n",
    "- **Arguments:**\n",
    "\n",
    "| Argument |                         Explanation                         |\n",
    "|:--------:|:-----------------------------------------------------------:|\n",
    "|   image  |<p style=\"text-align: left;\">image on which the map will be formed</p>|\n",
    "|   blur_kernel  |<p style=\"text-align: left;\">size of the Gaussian Kernel to be applied. </p>|\n",
    "|   color  |<p style=\"text-align: left;\">color of the obstacels to be filtered. </p>|\n",
    "| verbose  |<p style=\"text-align: left;\">Used for displaying the intermediate steps while debugging.</p>|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(cv2.cvtColor(wraped, cv2.COLOR_RGB2BGR))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "map=VisionProcessor.obstacles_map(wraped, color = 'pink', blur_kernel = (21, 21), verbose = False)\n",
    "\n",
    "plt.imshow(map)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### B) Using the Camera Feed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from vision import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the camera\n",
    "vp = VisionProcessor(camera_index=0)\n",
    "_ = vp.open()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Align the Field"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Color Filtering\n",
    "\n",
    "In order to fascilitate the use of color filtering a function called `color_filter` under the class `VisionProcessor` was createrd. The values of various colors in the HSV color space were preset in order to make the use of color filtering easier. Specifically the following colors where defined so far: red, green, blue, yellow, black, pink, white. If the color doesn't much the above an error is outputted saying so.\n",
    "\n",
    "`color_filtered_image = VisionProcessor.color_filter(img, color=color)`\n",
    "\n",
    "\n",
    "\n",
    "- **Output:**\n",
    "        Color filtered Image\n",
    "\n",
    "- **Arguments:**\n",
    "\n",
    "| Argument |                                          Explanation                                          |\n",
    "|:--------:|:---------------------------------------------------------------------------------------------:|\n",
    "|   image  |<p style=\"text-align: left;\"> image to be color filtered</p>|\n",
    "|   color  |<p style=\"text-align: left;\"> color in which to filter the image (i.e.red, green, blue, yellow, black, pink, white)</p> |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using Aruco Markers to detect corners\n",
    "\n",
    "In this part we get the pixel coordinates of all four corners by using the aruco markers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = cv2.imread(\"img/test.jpg\")\n",
    "corners, a_image = vp.visualize_aruco(img)\n",
    "corners\n",
    "plt.imshow(cv2.cvtColor(a_image, cv2.COLOR_RGB2BGR))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Perspective Projection\n",
    "\n",
    "The perspective projection section explains how the vision modules takes the four identified corners and returns a planar projection of the field. \n",
    "\n",
    "_`align_field`:_ This function which is part of the `VisionProcessor` class is responsible for taking the 4 corners identified before using the aruco markers and returning the perspecive transform `M` between the skewed plane of the field and the rectangle which is formed using the maximum Height and maximum width. To get the perpective transform a function from the OpenCV library is used called `cv2.getPerspectiveTransform` which uses the identified corners and the constructed rectangle corners.\n",
    "\n",
    "`M = VisionProcessor.align_field(corners)`\n",
    "\n",
    "\n",
    "\n",
    "- **Output:**\n",
    "        Perspective Transformation Matrix (M)\n",
    "\n",
    "- **Arguments:**\n",
    "\n",
    "| Argument |             Explanation            |\n",
    "|:--------:|:----------------------------------:|\n",
    "|  corners | corners array in pixel coordinates |\n",
    "\n",
    "\n",
    "_`warp`:_ This function which is part of the `VisionProcessor` class is responsible for taking an image and applying the previously calculated transformation. This is done by warping the image using the `cv2.wrapPerspective` function from the OpenCV library.\n",
    "\n",
    "`warped = VisionProcessor.warp(img, M)`\n",
    "\n",
    "- **Output:**\n",
    "        Warped image (warped)\n",
    "\n",
    "- **Arguments:**\n",
    "\n",
    "| Argument |                          Explanation                         |\n",
    "|:--------:|:------------------------------------------------------------:|\n",
    "|    img   |<p style=\"text-align: left;\">original image which will be wrapped with perspective matrix </p>|\n",
    "|     M    |<p style=\"text-align: left;\">perspective transformation matrix </p>                          |\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# corners = vp.corners_ar()\n",
    "M = VisionProcessor.align_field(corners)\n",
    "# img = vp._getImage()\n",
    "warped = vp.warp(img, M)\n",
    "pose = vp.get_robot_pose(warped, verbose=True)\n",
    "plt.imshow(cv2.cvtColor(warped, cv2.COLOR_RGB2BGR))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get the robot state with camera"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def camera_state(vp, M):\n",
    "    img = vp._getImage()\n",
    "    warped = vp.warp(img, M)\n",
    "    state = vp.get_robot_pose(warped)\n",
    "    return state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if camera_state(vp, M) is not None:\n",
    "    print('The Thymio is still on the map! Remove it!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Box Detection\n",
    "\n",
    "One of the expected outputs from the vision class is the pose of the robot. In order to be able to identify the pose of the robot two rectangular colored squares were placed on top of thymio. By finding the center of each of the boxes and connecting them we get a vector pointing to the direction where thymio is traveling.\n",
    "\n",
    "\n",
    "`(cx,cy) = VisionProcessor.detect_box(image,color='yellow',verbose=False)`\n",
    "\n",
    "\n",
    "- **Output:**\n",
    "        Cartesian Pixel Coordinates for the center of the box to be detected.\n",
    "        \n",
    "- **Arguments:**\n",
    "\n",
    "| Argument |                         Explanation                         |\n",
    "|:--------:|:-----------------------------------------------------------:|\n",
    "|   image  |<p style=\"text-align: left;\">image on which the box will be detected</p>|\n",
    "|   color  |<p style=\"text-align: left;\">color of the box to be filtered. </p>|\n",
    "| verbose  |<p style=\"text-align: left;\">Used for displaying the intermediate steps while debugging.</p>|\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_thymio = warped\n",
    "plt.imshow(cv2.cvtColor(img_thymio, cv2.COLOR_BGR2RGB))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(cx_blue,cy_blue)=VisionProcessor.detect_box(img_thymio,color='blue',verbose=False)\n",
    "(cx_yellow,cy_yellow)=VisionProcessor.detect_box(img_thymio,color='yellow',verbose=False)\n",
    "cv2.circle(img_thymio,(cx_blue,cy_blue),1,(0,255,0),12);\n",
    "cv2.circle(img_thymio,(cx_yellow,cy_yellow),1,(0,255,0),12);\n",
    "plt.imshow(cv2.cvtColor(img_thymio, cv2.COLOR_BGR2RGB))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ball Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wraped = VisionProcessor.warp(img, M)\n",
    "ball_pos=VisionProcessor.get_ball_xy(wraped,color = \"green\", minDist = 850,param1 = 19,param2 = 12,\n",
    "                                     minRadius = 42,maxRadius = 45,verbose = True)\n",
    "cv2.circle(wraped,ball_pos,1,(0,0,255),12);\n",
    "plt.imshow(cv2.cvtColor(wraped, cv2.COLOR_RGB2BGR))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build a Map\n",
    "\n",
    "_`obstacle_map`:_ This function is used to color filter obstacles in the `color` passed as argument and form a binary map of the field. The function receives as input the image in which the obstacles need to be detected. A Gaussian Blur filter is applied to smoothened the image. The scene is colored filtered with the given color argument in our case with the pink color. Erossion and dilation is applied to remove noise from the map.\n",
    "\n",
    "`map = VisionProcessor.obstacles_map(image, color = 'pink', blur_kernel = (19, 19), verbose = False)`\n",
    "\n",
    "\n",
    "- **Output:**\n",
    "        Binary Map of the image passed. Obstacles Represented in black color (map)\n",
    "        \n",
    "- **Arguments:**\n",
    "\n",
    "| Argument |                         Explanation                         |\n",
    "|:--------:|:-----------------------------------------------------------:|\n",
    "|   image  |<p style=\"text-align: left;\">image on which the map will be formed</p>|\n",
    "|   blur_kernel  |<p style=\"text-align: left;\">size of the Gaussian Kernel to be applied. </p>|\n",
    "|   color  |<p style=\"text-align: left;\">color of the obstacels to be filtered. </p>|\n",
    "| verbose  |<p style=\"text-align: left;\">Used for displaying the intermediate steps while debugging.</p>|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gMap = vp.getMap(warped)\n",
    "plt.imshow(gMap.obs_map)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Detect Objects"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The Field"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The obstacles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Thymio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "thymip_state_camera = None\n",
    "while thymip_state_camera is None:\n",
    "    thymip_state_camera = camera_state(vp, M)\n",
    "print(thymip_state_camera)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Global Navigation\n",
    "This module aims to plan a path from the start to the goal.\n",
    "- **Input**\n",
    "\n",
    "    - The map information\n",
    "\n",
    "        - Obstacles\n",
    "        - Start State(Thymio's current state)\n",
    "        - Goal State\n",
    "\n",
    "- **Output**\n",
    "\n",
    "    - A list of waypoints to reach the Goal State with Thymio's head\n",
    "\n",
    "**Parameters**\n",
    "\n",
    "| Name                | Meaning                                                      | Type (Unit) | Global |\n",
    "| :------------------- | :------------------------------------------------------------ | :----------- | :------ |\n",
    "| `map`        |  The `GridMap` consist of obstacles information              | GridMap         |  |\n",
    "| `method`      | the method to plan. \"A*\" or \"RRT\"             | String         |    |\n",
    "| `neighbor` | the number of neighbor. 4 or 6                    | Int         |   |\n",
    "| `path_simplification`         | do the path simplification or not | Bool |   |\n",
    "| `plot`         | plot the map or not | Bool |   |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from global_navigation import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre-Processing\n",
    "#### Enlarge the obstacles\n",
    "After getting the environment map, we need to enlarge the obstacles to make sure every point is safe for thymio."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "planner = PathPlanner(gMap, path_simplification=False)\n",
    "plt.imshow(planner.obs) # has auto enlarged the obs when load the map"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set the start point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pStart = thymip_state_camera.multiply(1/gMap.scale)\n",
    "pStart.pos.x = int(pStart.pos.x)\n",
    "pStart.pos.y = int(pStart.pos.y)\n",
    "planner.set_start(pStart)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Caculate the actual position\n",
    "The goal position represent where that the head(or the foot) of thmio will reach; We need to calculate the actual center position of thymio for the goal.\n",
    "\n",
    "* The default approaching angle is setted to `pi/2`\n",
    "* The default position of the head/foot is the front of Thymio(that is `Pos(ThymioSize + BallSize, 0)` in Thymio's coordination)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pGate = vp.getGate()\n",
    "pGate.x = int(pGate.x)\n",
    "pGate.y = int(pGate.y)\n",
    "print(\"Gate position\", pGate)\n",
    "pGoal2 = planner.approach(pGate)\n",
    "pBall = Pos(int(gMap.height/2), int(gMap.width/2))\n",
    "print(\"Ball position\", pBall)\n",
    "pGoal1 = planner.approach(pBall)\n",
    "print(\"To ball Goal State:\",pGoal1)\n",
    "planner.set_goal(pGoal1)\n",
    "print(\"To gate Goal State:\",pGoal2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This show how to approach to the ball\n",
    "import copy\n",
    "obs = copy.deepcopy(planner.obs)\n",
    "obs[(int)(pStart.pos.x)][(int)(pStart.pos.y)] = 0.3\n",
    "obs[pBall.x][pBall.y] = 0.7\n",
    "obs[pGoal1.pos.x][pGoal1.pos.y] = 0.5\n",
    "plt.imshow(obs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This show how to approach to the gate\n",
    "obs = copy.deepcopy(planner.obs)\n",
    "obs[(int)(pStart.pos.x)][(int)(pStart.pos.y)] = 0.3\n",
    "obs[pGate.x][pGate.y] = 0.7\n",
    "obs[pGoal2.pos.x][pGoal2.pos.y] = 0.5\n",
    "plt.imshow(obs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Path Planning\n",
    "We implemented two ways of path planning, namely, A* and RRT.\n",
    "\n",
    "* A* is an optimal path planning algorithm.\n",
    "* RRT is usually applied for high-dimension path planning. For our project, if we get a quite large grid map, the computational cost will be high, and RRT can boost up the speed. We should note that, it's not a optimal algorithms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the result of A*\n",
    "planner.method = \"A*\"\n",
    "planner.neighbor = 8\n",
    "apath = planner.plan()\n",
    "\n",
    "nobs = copy.deepcopy(obs)\n",
    "for p in apath:\n",
    "    nobs[p.x][p.y] = 0.9\n",
    "plt.imshow(nobs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the result of RRT\n",
    "planner.method = \"RRT\"\n",
    "rrtpath = planner.plan()\n",
    "\n",
    "nobs = copy.deepcopy(obs)\n",
    "for p in rrtpath:\n",
    "    nobs[p.x][p.y] = 0.9\n",
    "plt.imshow(nobs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Post-Processing\n",
    "#### Collect waypoints in same direction\n",
    "The waypoints in same direction just have the same effect for path tracking, so we try to elimate some redundent points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spath = planner.collect_wps(apath)\n",
    "\n",
    "nobs = copy.deepcopy(obs)\n",
    "for p in spath:\n",
    "    nobs[p.x][p.y] = 0.9\n",
    "plt.imshow(nobs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Shrink the waypoints\n",
    "Try to connect the grandparent to grandchild directly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "spath = planner.path_simplification(spath)\n",
    "\n",
    "nobs = copy.deepcopy(obs)\n",
    "for p in spath:\n",
    "    nobs[p.x][p.y] = 0.9\n",
    "    print(p)\n",
    "plt.imshow(nobs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Assign Orientation\n",
    "this function aims to provide more information about the path. \n",
    "\n",
    "* first it will tell the waypoints what the direction to the next. \n",
    "* second it insert a waypoint before the goal, so that it's able to not rotate anymore at the last waypoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "Global_path = planner.assign_ori(spath)\n",
    "\n",
    "for s in Global_path:\n",
    "    print(s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Local Navigation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The local navigation module allows to take advantage of the proximity sensors located on the five front horizontal proximity sensors. The objective is to bypass the unknown local obstacle for further re-computing of the controller to correct Thymio's speed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Input**\n",
    "\n",
    "    - Horizontal proximity sensor values\n",
    "\n",
    "\n",
    "- **Output**\n",
    "\n",
    "    - Motion control command for robot translation and rotation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Parameters**\n",
    "\n",
    "| Name                | Meaning                                                      | Type (Unit) | Global |\n",
    "| :------------------- | :------------------------------------------------------------ | :----------- | :------ |\n",
    "| `max_speed`        |  Nominal speed                 | Int         |  |\n",
    "| `obstThrL`      | Low obstacle threshold to switch state 1->0                | Int         |    |\n",
    "| `obstThrH` | High obstacle threshold to switch state 0->1                    | Int         |   |\n",
    "| `obstSpeedGain`         | Variation of speed according to the distance of obstacle | Int |   |\n",
    "| `state`         | 0=global navigation, 1=local navigation | Bool |   |\n",
    "| `obst`         | Measurements from left and right prox sensors | Int |   |\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Functions**\n",
    "\n",
    "Two functions are basically doing the local avoidance. obs is meant to detect an obstacle and return the new state of the robot as stated in the parameters tabular. obstacle_avoidance updates the new speed to return to thymio. It will return the inputs to the motors function. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "S_max_speed = 50       # nominal speed\n",
    "speedGain = 2      # gain used with ground gradient\n",
    "obstThrL = 10      # low obstacle threshold to switch state 1->0\n",
    "obstThrH = 20      # high obstacle threshold to switch state 0->1\n",
    "obstSpeedGain = 5  # /100 (actual gain: 5/100=0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tdmclient.notebook.sync_var\n",
    "def motors(left, right):\n",
    "    global motor_left_target, motor_right_target\n",
    "    motor_left_target = left\n",
    "    motor_right_target = right\n",
    "    \n",
    "@tdmclient.notebook.sync_var   \n",
    "def encoders():\n",
    "    global motor_left_speed, motor_right_speed\n",
    "    speed = []\n",
    "    while len(speed) < 2 :\n",
    "        speed = [motor_left_speed, motor_right_speed]\n",
    "    return speed\n",
    "    \n",
    "@tdmclient.notebook.sync_var\n",
    "def obstacle_avoidance():\n",
    "    global prox_horizontal, state, obst, obstSpeedGain, speed0, speedGain \n",
    "    obst = [prox_horizontal[0], prox_horizontal[4]]\n",
    "    \n",
    "    state = 0\n",
    "    if (obst[0] > obstThrH):\n",
    "        state = 1\n",
    "    elif (obst[1] > obstThrH):\n",
    "        state = 1\n",
    "        \n",
    "    if state == 1:\n",
    "        left_speed = max_speed // 2 + obstSpeedGain * (obst[0] // 100)\n",
    "        right_speed = max_speed // 2 + obstSpeedGain * (obst[1] // 100)\n",
    "        motors(left_speed, right_speed)\n",
    "        return True\n",
    "    else:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Filtering\n",
    "### Overview\n",
    "The goal of the filtering module is to integrate (if available) the measurements (in our case, the vision output) with the information from the motor encoders to estimate a reasonable state for Thymio. To this end, we use Extended Kalman Filter (EKF) technique in our project. Compared to the regular Kalman Filter, Extended Kalman Filter is the nonlinear version of the Kalman filter which utilizes first (or second) order derivative to do the approximation.\n",
    "\n",
    "## Robot States\n",
    "\n",
    "The state of Thymio is represented as $$\\mu_t = [x_t, y_t, \\theta_t]^T$$ as shown in the figure below, corresponding to the x, y, and orientation of the robot as shown in the figure below.\n",
    "<center><img src=\"Notebook_figures/robot_state.png\" alt=\"\" style=\"width: 800px;\"/></center>\n",
    "Where we denote the displacement in distance as D and displacement in orientation as T (for the sake of simplicity we omit the indices).\n",
    "\n",
    "## State Space Representation of EKF\n",
    "### Action and Measurement Model\n",
    "The action and measurement model of EKF is shown below, where $f$ and $h$ are two nonlinear functions.\n",
    "\\begin{array}{l}\\boldsymbol{x}_{t}=f\\left(\\boldsymbol{x}_{t-1}, \\boldsymbol{u}_{t}\\right)+\\boldsymbol{w}_{t} \\\\\\boldsymbol{z}_{t}=h\\left(\\boldsymbol{x}_{t}\\right)+\\boldsymbol{v}_{t}\\end{array}\n",
    "\n",
    "The prediction stage of EKF is described as:\n",
    "$$\\hat{\\boldsymbol{x}}_{t \\mid t-1}=f\\left(\\hat{\\boldsymbol{x}}_{t-1 \\mid t-1}, \\boldsymbol{u}_{t}\\right)$$\n",
    "\n",
    "\n",
    "where the input of the system is a vector consisting of two encoder displacements of Thymio left/right wheels represented as $u_t = [\\Delta S_r,\\Delta S_l ]^T$\n",
    "<center><img src=\"Notebook_figures/turning.png\" alt=\"\" style=\"width: 300px;\"/></center>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$x_{t+1}= f(x_t, u_t) = f(x_t, y_t, \\theta_t, \\Delta S{_r}, \\Delta S_l) =  \\begin{bmatrix} x_{t+1}\\\\ y_{t+1}\\\\\\theta_{t+1}  \\end{bmatrix} = \\begin{bmatrix} x_t + D cos(\\theta_t + \\frac{T}{2})\\\\ y_t + D sin(\\theta_t + \\frac{T}{2}) \\\\ \\theta_t + T  \\end{bmatrix}$$\n",
    " \n",
    "\n",
    "Since $f(x_t, u_t)$ is a non-linear function, we linearize it around the point ($\\mu_{x_t}$,$\\mu_{u_t}$). In order to do so, we needed to calculate the following two jacobian matrices:\n",
    "\n",
    "$$A = \\left.\\frac{\\partial f(x_t, u_t)}{\\partial x_t} \\right|_{(\\mu_{x_t},\\mu_{u_t})} = \\begin{bmatrix} \n",
    "1 & 0 & -D sin(\\theta_t + \\frac{T}{2}) \\\\ \n",
    "0 & 1 & D cos(\\theta_t + \\frac{T}{2})\\\\ \n",
    "0 & 0 & 1\n",
    "\\end{bmatrix}$$\n",
    "\n",
    "$$B = \\left.\\frac{\\partial f(x_t, u_t)}{\\partial u_t} \\right|_{(\\mu_{x_t},\\mu_{u_t})} = \\begin{bmatrix} \n",
    "\\frac{1}{2} cos(\\theta_t + \\frac{T}{2}) - \\frac{D}{2b} sin(\\theta_t + \\frac{T}{2}) & \\frac{1}{2} cos(\\theta_t + \\frac{T}{2}) + \\frac{D}{2b} sin(\\theta_t + \\frac{T}{2}) \\\\ \n",
    "\\frac{1}{2} sin(\\theta_t + \\frac{T}{2}) + \\frac{D}{2b} cos(\\theta_t + \\frac{T}{2}) & \\frac{1}{2} sin(\\theta_t + \\frac{T}{2}) - \\frac{D}{2b} cos(\\theta_t + \\frac{T}{2}) \\\\\n",
    "\\frac{1}{b} & - \\frac{1}{b}\n",
    "\\end{bmatrix}$$\n",
    "\n",
    "\n",
    "\n",
    "The control input had an associated covariance matrix $R= \\begin{bmatrix} k_r  & 0\\\\ 0 & k_l  \\end{bmatrix}$, where $k_r$ and $k_l$ are constants to be determined experimentally. \n",
    "\n",
    "In order to determine these constants, we measured the time Thymio took to travel a known distance, while recording the speed values in Thymio units. From these measurements we computed: $k_r = 2.5 \\cdot 10^{-2} m$ and $k_l = 1.5 \\cdot 10^{-2} m$.\n",
    "\n",
    "\n",
    "The prediction step is:\n",
    "- $\\overline{\\mu}_{t} = f(\\mu_{x_{t-1}},\\mu_{u_{t-1}})$\n",
    "- $\\overline{\\Sigma}_{t} = A \\Sigma_{t-1} A^T + B R B^T$\n",
    "\n",
    "\n",
    "## Measurement model\n",
    "\n",
    "\n",
    "We chose the measurement model to be the following :\n",
    "\n",
    "$y_t = H_t x_t + \\epsilon_t = \\begin{bmatrix} 1 & 0 & 0\\\\ 0 & 1 & 0\\\\ 0 & 0 & 1 \\end{bmatrix} \\begin{bmatrix} x_t\\\\ y_t\\\\\\theta_t  \\end{bmatrix} + \\epsilon_t$, where $\\epsilon_t$ is the measurment noise with zero mean and covariance $Q = \\begin{bmatrix} q_x & 0 & 0\\\\ 0 & q_y & 0 \\\\ 0 & 0 & q_{\\theta}  \\end{bmatrix} $, where $q_x$, $q_y$ and $q_t$ are constants to be determined experimentally.\n",
    "\n",
    "In order to determine these constants, we measured several known positions of Thymio using the camera. From these measurements we computed: $q_x = 2.8948 \\cdot 10^{-4} m$, $q_y = 8.2668 \\cdot 10^{-4} m$ and $q_{\\theta} = 0.0029 rad$.\n",
    "\n",
    "\n",
    "$z_t = \\begin{bmatrix} z_{x_t}\\\\ z_{y_t}\\\\ z_{\\theta_t}  \\end{bmatrix}$ is the reading from the camera (exteroceptive sensor), also called the observation.\n",
    "\n",
    "The kalman gain is defined as $K_t = \\overline{\\Sigma}_{t}H_t^T(H_t \\overline{\\Sigma}_{t} H_t^T + Q)^{-1} = \\overline{\\Sigma}_{t}( \\overline{\\Sigma}_{t} + Q)^{-1}$. \n",
    "\n",
    "Hence, the updated step is :\n",
    "- $\\mu_{t} = \\overline{\\mu}_{t} + K_t(z_t - \\overline{\\mu}_{t})$\n",
    "- $\\Sigma_{t} = (I-K_tH_t) \\overline{\\Sigma}_{t} = (I-K_t) \\overline{\\Sigma}_{t}$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import filtering\n",
    "# Motor Calibration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pre_state = np.array([thymip_state_camera.pos.x, thymip_state_camera.pos.y, thymip_state_camera.ori]).reshape(-1, 1) # initial state\n",
    "pre_cov = np.ones([3, 3]) * 0.03 # initial covariance\n",
    "G_filter = filtering.KF(pre_state, pre_cov, qx=0.1, qy=0.1, qtheta=0.1, rl=0.1, rr=0.1, b=0.0927)\n",
    "G_filter.timer = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "G_filter.states"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Motion Control\n",
    "\n",
    "Motion Control part deals with the motion control and serves as a interface for other program to Thymio."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Parameters**\n",
    "\n",
    "| Name                | Meaning                                                      | Type (Unit) | Global |\n",
    "| :------------------- | :------------------------------------------------------------ | :----------- | :------ |\n",
    "| `S_max_speed`        |  Maxium Speed of motion                 | Int         |  |\n",
    "| `S_track_interval`      | The time interval to control the thymio to track the path      | Float(s)         |    |\n",
    "| `S_speed_scale` | The speed factor of advance to real speed(m/s)                    | Float(s/m)         |   |\n",
    "| `S_rotate_scale`         | The speed factor of rotation to real speed(rad/s)  | Float(s) |   |\n",
    "| `S_epsilon_dis`         | the maximium error of postion we accept | Float(m) |   |\n",
    "| `S_epsilon_theta`         | the maximium error of orientation we accept | Float(m) |   |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "S_track_interval = 0.15 \n",
    "S_speed_scale = 0.000315\n",
    "S_rotate_scale = 0.1\n",
    "S_epsilon_dis = 0.01\n",
    "S_epsilon_theta = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MotionController_JN:\n",
    "    def __init__(self, \n",
    "                 time_interval = S_track_interval, # s \n",
    "                 eps_delta_r = S_epsilon_dis, \n",
    "                 eps_delta_theta = S_epsilon_theta,\n",
    "                 max_speed = S_max_speed, \n",
    "                 speed_scale = S_speed_scale, # (m/s) / speed_in_motor_command; 0.000315 for speed<200; 0.0003 for speed \\in (200,400)\n",
    "                 rotate_scale = S_rotate_scale, # TODO (rad/s) / speed_in_motor_command\n",
    "                 verbose = False\n",
    "                 ):\n",
    "        \"\"\"Motion Controller\n",
    "\n",
    "        * Interface between high-level command and Thymio motion\n",
    "        * Thymio motion control\n",
    "        \"\"\"\n",
    "        self.interval = time_interval   # s, control frequency\n",
    "        self.displacement = [0, 0]\n",
    "        \n",
    "        self.eps_delta_r = eps_delta_r\n",
    "        self.eps_delta_theta = eps_delta_theta\n",
    "\n",
    "        self.max_speed = max_speed\n",
    "        self.speed_scale = speed_scale\n",
    "        self.rotate_scale = rotate_scale\n",
    "        \n",
    "        self.timer = time.time() # timer to calculate the displacement with speed*time\n",
    "\n",
    "        self.verbose = verbose\n",
    "\n",
    "    # -- Local Navigation --\n",
    "    def avoid(self):\n",
    "        \"\"\"do local navigation, and return if there's obstacle\"\"\"\n",
    "        return obstacle_avoidance()\n",
    "\n",
    "    # -- Path Tracking --        \n",
    "    def path_tracking(self, waypoint, Thymio_state, theta_track = False):\n",
    "        \"\"\"Try to reach the waypoint\n",
    "\n",
    "        @return(Bool): waypoint reached\n",
    "        \"\"\"\n",
    "        # Are we close enough to the next waypoint?  \n",
    "        delta_r = Thymio_state.dis(waypoint)\n",
    "        if delta_r < self.eps_delta_r:\n",
    "            if self.verbose:\n",
    "                print(\"Close to the point\")\n",
    "            # check the rotation\n",
    "            delta_theta = Thymio_state.delta_theta(waypoint)\n",
    "            if not theta_track or abs(delta_theta) < self.eps_delta_theta:\n",
    "                if self.verbose:\n",
    "                    print(Thymio_state,\"Point Finished\")\n",
    "                return True\n",
    "            else:\n",
    "                self.rotate(delta_theta) \n",
    "        else:\n",
    "            # Go to the next waypoint\n",
    "            headto_theta = Thymio_state.headto(waypoint)\n",
    "            delta_theta = headto_theta - Thymio_state.ori\n",
    "            delta_theta = Pos.projectin2pi(delta_theta)\n",
    "            if self.verbose:\n",
    "                print(F\"headto_theta: {headto_theta}\")\n",
    "            if abs(delta_theta) > self.eps_delta_theta:#1.0:\n",
    "                self.rotate(delta_theta)\n",
    "            # if abs(delta_theta) > 1.0:\n",
    "            #    self.rotate(delta_theta)\n",
    "            # elif abs(delta_theta) > self.eps_delta_theta:\n",
    "            #     self.approach(delta_r, delta_theta)\n",
    "            else:\n",
    "                self.approach(delta_r, 0)\n",
    "            return False\n",
    "\n",
    "    # -- Movement --\n",
    "    def approach(self, delta_r, delta_theta = 0, base_speed = 20):\n",
    "        \"\"\"approach to the goal point\n",
    "        \n",
    "            move with modification of direction\n",
    "        \"\"\"\n",
    "        if self.verbose:\n",
    "            print(F\"approach to dr:{delta_r, :.3f}, dt:{delta_theta, :.2f}\")\n",
    "        # assume we only move <interval> s. \n",
    "        advance_speed = min(delta_r/self.interval/self.speed_scale * 10 + base_speed, self.max_speed)\n",
    "        delta_speed = delta_theta/self.interval/self.rotate_scale\n",
    "        if delta_speed > 0:\n",
    "            delta_speed = min(delta_speed, self.max_speed/2)\n",
    "            self.move(min(advance_speed, self.max_speed - 2*abs(delta_speed)), delta_speed)\n",
    "        else:\n",
    "            delta_speed = max(delta_speed, -self.max_speed/2)\n",
    "            self.move(min(advance_speed, self.max_speed - 2*abs(delta_speed)), delta_speed)\n",
    "\n",
    "    def rotate(self, delta_theta, base_speed = 5):\n",
    "        \"\"\"rotate in place\n",
    "        \"\"\"\n",
    "        if self.verbose:\n",
    "            print(F\"rotate to dt:{delta_theta, :.2f}\")\n",
    "        delta_speed = delta_theta/(self.interval)/self.rotate_scale\n",
    "        if delta_speed > 0:\n",
    "            self.move(0, min(delta_speed + base_speed, self.max_speed))\n",
    "        else:\n",
    "            self.move(0, max(delta_speed - base_speed, -self.max_speed))\n",
    "\n",
    "    def move(self, vel, omega = 0):\n",
    "        \"\"\"\n",
    "        move with transitional velocity and rotational velocity\n",
    "        \"\"\"\n",
    "        if self.verbose:\n",
    "            print(F\"move with {vel, :.0f}, {omega, :.0f}\")\n",
    "        self._set_motor(vel - omega, vel + omega)\n",
    "\n",
    "    def stop(self):\n",
    "        \"\"\"Stop both motors\n",
    "        \"\"\"\n",
    "        self._set_motor(0, 0)\n",
    "\n",
    "    def update_displacement(self):\n",
    "        starter = time.time()\n",
    "        interval = starter - self.timer\n",
    "        self.timer = starter\n",
    "        rls, rrs = encoders()\n",
    "        # rls = int(rls * float(os.getenv(\"OFFSET_WHEELS\")))\n",
    "        # !deal with irregular value!\n",
    "        rls = 0 if abs(rls) > self.max_speed * 1.1 else rls\n",
    "        rrs = 0 if abs(rrs) > self.max_speed * 1.1 else rrs \n",
    "        self.displacement[0] += rls*interval*self.speed_scale\n",
    "        self.displacement[1] += rrs*interval*self.speed_scale\n",
    "\n",
    "    def _set_motor(self, ls, rs):\n",
    "        ls = (int)(ls)\n",
    "        rs = (int)(rs)\n",
    "        # l_speed = int(ls / float(os.getenv(\"OFFSET_WHEELS\")))\n",
    "        self.update_displacement()\n",
    "        motors(ls, rs)\n",
    "\n",
    "    def get_displacement(self):\n",
    "        self.update_displacement()\n",
    "        ret = self.displacement\n",
    "        self.displacement = [0, 0]\n",
    "        # if self.verbose:\n",
    "        #     print(F\"Displacement:{ret}\")\n",
    "        return ret\n",
    "\n",
    "    \n",
    "G_mc = MotionController_JN(verbose = G_verbose)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this main all the motion control and local avoidance functions are called in a while loop. This merges all the modules and communicates with the thymio thanks to the motion contol module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(camera_state(vp, M))\n",
    "G_mc.get_displacement()\n",
    "G_filter.timer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(camera_state(vp, M))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(gMap.goal.multiply(gMap.scale))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "S_camera_interval = 0.1 #s\n",
    "G_track_timer = time.time()\n",
    "G_camera_timer = time.time()\n",
    "G_mc.get_displacement() # clear the displacement stored\n",
    "\n",
    "\n",
    "def localizate(starter):\n",
    "    \"\"\"Track Where Thymio is\"\"\"\n",
    "    global G_camera_timer\n",
    "    # 3. Localization \n",
    "    # 3.1 odometer\n",
    "    dsl, dsr = G_mc.get_displacement()\n",
    "    # 3.2 With Vision\n",
    "    if starter - G_camera_timer > S_camera_interval:\n",
    "        vision_thymio_state = camera_state(vp, M)\n",
    "        print('Using camera to get state, state is: ')\n",
    "        print(vision_thymio_state)\n",
    "        if vision_thymio_state is None:\n",
    "            G_filter.kalman_filter(dsr, dsl)\n",
    "        else:\n",
    "            G_camera_timer = starter\n",
    "            G_filter.kalman_filter(dsr, dsl, vision_thymio_state)\n",
    "    else:\n",
    "        G_filter.kalman_filter(dsr, dsl)\n",
    "        \n",
    "    G_filter.plot_gaussian(factor=1000, dt=1)\n",
    "    thymio_state = G_filter.get_state()\n",
    "#     print('Current state: ', thymio_state)\n",
    "    return thymio_state\n",
    "\n",
    "print(localizate(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reset():\n",
    "    Global_path = planner.assign_ori(spath)\n",
    "    Goal_state = Global_path[-1]\n",
    "    G_filter = filtering.KF(pre_state, pre_cov, qx=0.1, qy=0.1, qtheta=0.1, rl=0.1, rr=0.1, b=0.0927)\n",
    "    G_track_timer = time.time()\n",
    "    G_camera_timer = time.time()\n",
    "    G_mc.get_displacement() # clear the displacement stored\n",
    "    print(localizate(0))\n",
    "\n",
    "## Fake Waypoints\n",
    "# Global_path = [State(Pos(0.2, 0.1),1.51), State(Pos(0.3, 0.3),1.5), State(Pos(0.2, 0.4),1.5), \n",
    "#                State(Pos(0.3, 0.5),1.5), State(Pos(0.3,0.6),1.7)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "reset()\n",
    "debug_timer = 0.0\n",
    "while True:\n",
    "    starter = time.time()\n",
    "    # 3. Localization\n",
    "    Thymio_state = localizate(starter)\n",
    "    # 2.2.1 Finished?\n",
    "    if len(Global_path) == 0:\n",
    "        G_mc.stop()\n",
    "        if G_verbose:\n",
    "            print(\"Terminate Reached!\")\n",
    "        break\n",
    "    print(\"Next waypoint is: \", Global_path[0])\n",
    "    # 2.2.2 Is there obstacles on the front?\n",
    "    obs_front = G_mc.avoid() # do local navigation for, like, 10ms\n",
    "    #     # TODO: replan\n",
    "    if (not obs_front) and starter - G_track_timer > S_track_interval:\n",
    "        # 4. Follow the path    # <-- The only task can run under low frequency\n",
    "        reached = G_mc.path_tracking(Global_path[0], Thymio_state)#, len(Global_path) == 1)\n",
    "        if reached:\n",
    "            print(Global_path[0],\"reached\")\n",
    "            Global_path = Global_path[1:]\n",
    "            # assume Global_path is not empty because of 2.2.1\n",
    "        G_track_timer = starter\n",
    "    loop_time = time.time() - starter\n",
    "    time.sleep(0.1)\n",
    "    print(loop_time)\n",
    "    if starter - debug_timer > S_track_interval:\n",
    "        debug_timer = starter\n",
    "        print(F\"thymio: {Thymio_state}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "motors(0,0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#vp.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for s in Global_path:\n",
    "    print(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "b10cf93940e5c0473413f3fc99b95e2c1997f6e8852312471085cb7234ec8d25"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
